---
title: "Assignment 5"
author: "Robert Evans"
date: "April 16, 2017"
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#Chapter 6- Clustering
This chapter goes over four different types of clustering

* k-Means Clustering
* k-Medoids Clustering
* Hierarchical Clustering
* Density-based Clustering

##Section 6.1- k-Means Clustering
This section goes over how to set up a k-Means cluster of the iris data set
```{r}
# Load the iris data set into iris
iris2 <- iris

#remove the species variable from the data
iris2$Species <- NULL

#setting a seed to get repeatble results
set.seed(123)
#apply the kmeans tothe iris2 data set. The 3 sets the cluster count to 3
(kmeans.result <- kmeans(iris2, 3))

#compare the actual classifications with the predicted clustering result using a confusion matrix
table(iris$Species, kmeans.result$cluster)
#after running the above code its easy to see that the setosa species is easy to separate from the other species.

#plot the species clusters with Sepal.Length and Sepal.Width
plot(iris2[c("Sepal.Length", "Sepal.Width")], col = kmeans.result$cluster)
points(kmeans.result$centers[,c("Sepal.Length", "Sepal.Width")], col = 1:3,
       pch = 8, cex=2)
#after looking at the graph you can see that the Setosa species (black points) are easily separated while the versicolor and virginica species have some overlap

```

##6.2- k-Medoids Clustering
This uses the k-mediods method of clustering. This is similar k-means with the exception that k-medoids is represented by the object that is the closest to the center of the cluster
```{r}
#install.packages("fpc")
#install.packages("cluster")

library(fpc)
library(cluster)

pamk.result <- pamk(iris2)

#number of clusters
pamk.result$nc

#check cluster against actual results
table(pamk.result$pamobject$clustering, iris$Species)

#display cluster as a graph
layout(matrix(c(1,2),1,2)) # displays 2 graphs on one row
plot(pamk.result$pamobject)
layout(matrix(1)) # change back to one graph per page

#rerun pam but with 3 clusters
pam.result <- pam(iris2, 3)
table(pam.result$clustering, iris$Species)

#display cluster as a graph
layout(matrix(c(1,2),1,2)) # displays 2 graphs on one row
plot(pam.result)
layout(matrix(1)) # change back to one graph per page
```

##6.3 Hierarchical Clustering
This section creates a clustering model based off of a hierarchical or tree set up
```{r}
#get a sample of 40 records and remove the species variable
idx <- sample(1:dim(iris)[1], 40)
irisSample <- iris[idx,]
irisSample$Species <- NULL

#create the cluster
hc <- hclust(dist(irisSample), method="ave")

#plot the cluster
plot(hc, hang = -1, labels=iris$Species[idx])
```

##6.4- Density-based clustering
DBSCAN creates a density-based clustering of numberic data. This method clusters objects into one cluster if they are connected by densley populated data
```{r}
#remove classifications
iris2 <- iris[-5]
#generate classification
ds <- dbscan(iris2, eps=0.42, MinPts=5)

#check results- in this table 0 is representative of outliers
table(ds$cluster, iris$Species)

plot(ds,iris2)

#show clusters with representation of sepal.length vs petal.width
plot(ds, iris2[c(1,4)])

#show data classifications and outliers
plotcluster(iris2, ds$cluster)

#create a new data set
set.seed(435)
idx <- sample(1:nrow(iris), 10)
newData <- iris[idx,-5]
newData <- newData + matrix(runif(10*4, min=0, max=0.2), nrow=10, ncol=4)
#label the data
myPred <- predict(ds, iris2, newData)
#check the labels
plot(iris2[c(1,4)], col=1+ds$cluster)
points(newData[c(1,4)], pch="*", col=1+myPred, cex=3)
#check the cluster labels
table(myPred, iris$Species[idx])

```


